{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fd724922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T07:37:22.767706Z",
     "start_time": "2024-02-04T07:37:22.057693Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd # Library for Dataframes \n",
    "import numpy as np # Library for math functions\n",
    "import pickle # Python object serialization library. Not secure\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332408f",
   "metadata": {},
   "source": [
    "# TEXT REPRESENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd9e20",
   "metadata": {},
   "source": [
    "## Basic Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a671b47",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5353ec0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:10:45.136838Z",
     "start_time": "2024-02-04T05:10:45.121264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36de0c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:11:37.352244Z",
     "start_time": "2024-02-04T05:11:37.340974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n"
     ]
    }
   ],
   "source": [
    "#Build the vocabulary\n",
    "vocab = {}\n",
    "count = 0\n",
    "for doc in processed_docs:\n",
    "    for word in doc.split():\n",
    "        if word not in vocab:\n",
    "            count = count +1\n",
    "            vocab[word] = count\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6dd7929",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:11:38.964764Z",
     "start_time": "2024-02-04T05:11:38.955609Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_onehot_vector(somestring):\n",
    "    onehot_encoded = []\n",
    "    for word in somestring.split():\n",
    "        temp = [0]*len(vocab)\n",
    "        if word in vocab:\n",
    "            temp[vocab[word]-1] = 1 # -1 is to take care of the fact indexing in array starts from 0 and not 1\n",
    "        onehot_encoded.append(temp)\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "073e875b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:11:54.056676Z",
     "start_time": "2024-02-04T05:11:54.047177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man bites dog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(processed_docs[1])\n",
    "get_onehot_vector(processed_docs[1]) #one hot representation for a text from our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba001f4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:12:50.065173Z",
     "start_time": "2024-02-04T05:12:49.387088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data:  ['dog', 'bites', 'man', 'man', 'bites', 'dog', 'dog', 'eats', 'meat', 'man', 'eats', 'food']\n",
      "Label Encoded: [1 0 4 4 0 1 1 2 5 4 2 3]\n",
      "Onehot Encoded Matrix:\n",
      " [[1. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding using scikit-learn\n",
    "\n",
    "S1 = 'dog bites man'\n",
    "S2 = 'man bites dog'\n",
    "S3 = 'dog eats meat'\n",
    "S4 = 'man eats food'\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "data = [S1.split(), S2.split(), S3.split(), S4.split()]\n",
    "values = data[0]+data[1]+data[2]+data[3]\n",
    "print(\"The data: \",values)\n",
    "\n",
    "#Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(\"Label Encoded:\",integer_encoded)\n",
    "\n",
    "#One-Hot Encoding\n",
    "onehot_encoder = OneHotEncoder()\n",
    "onehot_encoded = onehot_encoder.fit_transform(data).toarray()\n",
    "print(\"Onehot Encoded Matrix:\\n\",onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e37b16",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79895eea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:14:18.355700Z",
     "start_time": "2024-02-04T05:14:18.319304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus:  ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
      "Our vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
      "BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n",
      "Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#look at the documents list\n",
    "print(\"Our corpus: \", processed_docs)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad5d63",
   "metadata": {},
   "source": [
    "In the above code, we represented the text considering the frequency of words into account. However, sometimes, we don't care about frequency much, but only want to know whether a word appeared in a text or not. That is, each document is represented as a vector of 0s and 1s. We will use the option binary=True in CountVectorizer for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5bac0a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:15:07.487364Z",
     "start_time": "2024-02-04T05:15:07.480131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#BoW with binary vectors\n",
    "count_vect = CountVectorizer(binary=True)\n",
    "count_vect.fit(processed_docs)\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863092d7",
   "metadata": {},
   "source": [
    "### Bag of N-gram words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa49cd24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:16:00.071956Z",
     "start_time": "2024-02-04T05:16:00.059587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n",
      "BoW representation for 'dog bites man':  [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n",
      "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Ngram vectorization example with count vectorizer and uni, bi, trigrams\n",
    "count_vect = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b5a4f",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65adb82d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:16:56.167909Z",
     "start_time": "2024-02-04T05:16:56.141628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for all words in the vocabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
      "----------\n",
      "All words in the vocabulary ['bites', 'dog', 'eats', 'food', 'man', 'meat']\n",
      "----------\n",
      "TFIDF representation for all documents in our corpus\n",
      " [[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
      " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
      " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
      " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
      "----------\n",
      "Tfidf representation for 'dog and man are friends':\n",
      " [[0.         0.70710678 0.         0.         0.70710678 0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanket.p/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(processed_docs)\n",
    "\n",
    "#IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "print(\"-\"*10)\n",
    "#All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names())\n",
    "print(\"-\"*10)\n",
    "\n",
    "#TFIDF representation for all documents in our corpus \n",
    "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray()) \n",
    "print(\"-\"*10)\n",
    "\n",
    "temp = tfidf.transform([\"dog and man are friends\"])\n",
    "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac4ed5",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c195a409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:40:44.806937Z",
     "start_time": "2024-02-04T05:40:44.791177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading subset of some pretrained embeddings (Google News which has vector size of 300 with ~3 million words)\n",
    "path_word_embedding = \"/Users/sanket.p/python_workbooks/sanket_workbooks/Coursera Courses/Natural-Language-Processing-Specialization-master/Natural Language Processing with Classification and Vector Spaces/Week 3/word_embeddings_subset.p\"\n",
    "word_embeddings = pickle.load( open( path_word_embedding, \"rb\" ) )\n",
    "len(word_embeddings) # there should be 243 words that will be used in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18ab8a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:21:28.507193Z",
     "start_time": "2024-02-04T05:21:28.490085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[-0.08007812  0.13378906  0.14355469  0.09472656 -0.04736328 -0.02355957\n",
      " -0.00854492 -0.18652344  0.04589844 -0.08154297 -0.03442383 -0.11621094\n",
      "  0.21777344 -0.10351562 -0.06689453  0.15332031 -0.19335938  0.26367188\n",
      " -0.13671875 -0.05566406  0.07470703 -0.00070953  0.09375    -0.14453125\n",
      "  0.04296875 -0.01916504 -0.22558594 -0.12695312 -0.0168457   0.05224609\n",
      "  0.0625     -0.1484375  -0.01965332  0.17578125  0.10644531 -0.04760742\n",
      " -0.10253906 -0.28515625  0.10351562  0.20800781 -0.07617188 -0.04345703\n",
      "  0.08642578  0.08740234  0.11767578  0.20996094 -0.07275391  0.1640625\n",
      " -0.01135254  0.0025177   0.05810547 -0.03222656  0.06884766  0.046875\n",
      "  0.10107422  0.02148438 -0.16210938  0.07128906 -0.16210938  0.05981445\n",
      "  0.05102539 -0.05566406  0.06787109 -0.03759766  0.04345703 -0.03173828\n",
      " -0.03417969 -0.01116943  0.06201172 -0.08007812 -0.14941406  0.11914062\n",
      "  0.02575684  0.00302124  0.04711914 -0.17773438  0.04101562  0.05541992\n",
      "  0.00598145  0.03027344 -0.07666016 -0.109375    0.02832031 -0.10498047\n",
      "  0.0100708  -0.03149414 -0.22363281 -0.03125    -0.01147461  0.17285156\n",
      "  0.08056641 -0.10888672 -0.09570312 -0.21777344 -0.07910156 -0.10009766\n",
      "  0.06396484 -0.11962891  0.18652344 -0.02062988 -0.02172852  0.29296875\n",
      " -0.00793457  0.0324707  -0.15136719  0.00227356 -0.03540039 -0.13378906\n",
      "  0.0546875  -0.03271484 -0.01855469 -0.10302734 -0.13378906  0.11425781\n",
      "  0.16699219  0.01361084 -0.02722168 -0.2109375   0.07177734  0.08691406\n",
      " -0.09960938  0.01422119 -0.18261719  0.00741577  0.01965332  0.00738525\n",
      " -0.03271484 -0.15234375 -0.26367188 -0.14746094  0.03320312 -0.03344727\n",
      " -0.01000977  0.01855469  0.00183868 -0.10498047  0.09667969  0.07910156\n",
      "  0.11181641  0.13085938 -0.08740234 -0.1328125   0.05004883  0.19824219\n",
      "  0.0612793   0.16210938  0.06933594  0.01281738  0.01550293  0.01531982\n",
      "  0.11474609  0.02758789  0.13769531 -0.08349609  0.01123047 -0.20507812\n",
      " -0.12988281 -0.16699219  0.20410156 -0.03588867 -0.10888672  0.0534668\n",
      "  0.15820312 -0.20410156  0.14648438 -0.11572266  0.01855469 -0.13574219\n",
      "  0.24121094  0.12304688 -0.14550781  0.17578125  0.11816406 -0.30859375\n",
      "  0.10888672 -0.22363281  0.19335938 -0.15722656 -0.07666016 -0.09082031\n",
      " -0.19628906 -0.23144531 -0.09130859 -0.14160156  0.06347656  0.03344727\n",
      " -0.03369141  0.06591797  0.06201172  0.3046875   0.16796875 -0.11035156\n",
      " -0.03833008 -0.02563477 -0.09765625  0.04467773 -0.0534668   0.11621094\n",
      " -0.15039062 -0.16308594 -0.15527344  0.04638672  0.11572266 -0.06640625\n",
      " -0.04516602  0.02331543 -0.08105469 -0.0255127  -0.07714844  0.0016861\n",
      "  0.15820312  0.00994873 -0.06445312  0.15722656 -0.03112793  0.10644531\n",
      " -0.140625    0.23535156 -0.11279297  0.16015625  0.00061798 -0.1484375\n",
      "  0.02307129 -0.109375    0.05444336 -0.14160156  0.11621094  0.03710938\n",
      "  0.14746094 -0.04199219 -0.01391602 -0.03881836  0.02783203  0.10205078\n",
      "  0.07470703  0.20898438 -0.04223633 -0.04150391 -0.00588989 -0.14941406\n",
      " -0.04296875 -0.10107422 -0.06176758  0.09472656  0.22265625 -0.02307129\n",
      "  0.04858398 -0.15527344 -0.02282715 -0.04174805  0.16699219 -0.09423828\n",
      "  0.14453125  0.11132812  0.04223633 -0.16699219  0.10253906  0.16796875\n",
      "  0.12597656 -0.11865234 -0.0213623  -0.08056641  0.24316406  0.15527344\n",
      "  0.16503906  0.00854492 -0.12255859  0.08691406 -0.11914062 -0.02941895\n",
      "  0.08349609 -0.03100586  0.13964844 -0.05151367  0.00765991 -0.04443359\n",
      " -0.04980469 -0.03222656 -0.00952148 -0.10888672 -0.10302734 -0.15722656\n",
      "  0.19335938  0.04858398  0.015625   -0.08105469 -0.11621094 -0.01989746\n",
      "  0.05737305  0.06103516 -0.14550781  0.06738281 -0.24414062 -0.07714844\n",
      "  0.04760742 -0.07519531 -0.14941406 -0.04418945  0.09716797  0.06738281]\n"
     ]
    }
   ],
   "source": [
    "countryVector = word_embeddings['country'] # give vector representation for word country\n",
    "print(type(countryVector))\n",
    "print(countryVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ed9a74",
   "metadata": {},
   "source": [
    "To look for the closest words in the embedding that matchess the candidate country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "711ddef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:32:13.926535Z",
     "start_time": "2024-02-04T05:32:13.921548Z"
    }
   },
   "outputs": [],
   "source": [
    "def vec(word):\n",
    "    return word_embeddings[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c9babca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:32:15.002402Z",
     "start_time": "2024-02-04T05:32:14.954002Z"
    }
   },
   "outputs": [],
   "source": [
    "keys = word_embeddings.keys()\n",
    "data = []\n",
    "for key in keys:\n",
    "    data.append(word_embeddings[key])\n",
    "    \n",
    "embedding = pd.DataFrame(data=data, index=keys)\n",
    "\n",
    "def find_closest_word(v, k =1):\n",
    "    '''\n",
    "    Function to find the closest word to vector\n",
    "    '''\n",
    "    # calculate vector diff from each word to input vector\n",
    "    diff = embedding.values - v \n",
    "    \n",
    "    # calculate norm of each diff vector\n",
    "    # (means squared euclidean dist from each word to input vector)\n",
    "    delta = np.sum(diff * diff, axis=1)\n",
    "    \n",
    "    #Get index of minimum dist in the array\n",
    "    i = np.argmin(delta)\n",
    "    \n",
    "    # return row name for this item\n",
    "    return embedding.iloc[i].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fac180fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:32:15.462037Z",
     "start_time": "2024-02-04T05:32:15.424584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>-0.080078</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.023560</td>\n",
       "      <td>-0.008545</td>\n",
       "      <td>-0.186523</td>\n",
       "      <td>0.045898</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>-0.244141</td>\n",
       "      <td>-0.077148</td>\n",
       "      <td>0.047607</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>-0.149414</td>\n",
       "      <td>-0.044189</td>\n",
       "      <td>0.097168</td>\n",
       "      <td>0.067383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>-0.010071</td>\n",
       "      <td>0.057373</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>-0.029785</td>\n",
       "      <td>-0.079102</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.013306</td>\n",
       "      <td>-0.143555</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024292</td>\n",
       "      <td>-0.168945</td>\n",
       "      <td>-0.062988</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>-0.020508</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>-0.247070</td>\n",
       "      <td>-0.122559</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>-0.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>-0.073242</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.108887</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>-0.127930</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>-0.045654</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>0.034424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.079590</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.037842</td>\n",
       "      <td>-0.183594</td>\n",
       "      <td>0.137695</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>-0.079590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iraq</th>\n",
       "      <td>0.191406</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>0.060059</td>\n",
       "      <td>-0.285156</td>\n",
       "      <td>-0.102539</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>-0.351562</td>\n",
       "      <td>-0.095215</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100586</td>\n",
       "      <td>-0.077148</td>\n",
       "      <td>-0.123047</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>-0.153320</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-0.173828</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>0.302734</td>\n",
       "      <td>0.105957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil</th>\n",
       "      <td>-0.139648</td>\n",
       "      <td>0.062256</td>\n",
       "      <td>-0.279297</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.044434</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>-0.184570</td>\n",
       "      <td>-0.498047</td>\n",
       "      <td>0.047363</td>\n",
       "      <td>0.110840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.195312</td>\n",
       "      <td>-0.345703</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.091797</td>\n",
       "      <td>0.051025</td>\n",
       "      <td>0.061279</td>\n",
       "      <td>0.194336</td>\n",
       "      <td>0.204102</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>-0.051025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Belmopan</th>\n",
       "      <td>-0.265625</td>\n",
       "      <td>-0.380859</td>\n",
       "      <td>-0.049072</td>\n",
       "      <td>0.155273</td>\n",
       "      <td>-0.044922</td>\n",
       "      <td>-0.248047</td>\n",
       "      <td>-0.010376</td>\n",
       "      <td>-0.105957</td>\n",
       "      <td>-0.328125</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047119</td>\n",
       "      <td>-0.034424</td>\n",
       "      <td>-0.005219</td>\n",
       "      <td>-0.265625</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.170898</td>\n",
       "      <td>-0.353516</td>\n",
       "      <td>0.072754</td>\n",
       "      <td>-0.042969</td>\n",
       "      <td>0.229492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vaduz</th>\n",
       "      <td>0.324219</td>\n",
       "      <td>-0.056885</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>-0.045898</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>-0.062256</td>\n",
       "      <td>0.004089</td>\n",
       "      <td>-0.328125</td>\n",
       "      <td>-0.151367</td>\n",
       "      <td>0.242188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134766</td>\n",
       "      <td>-0.226562</td>\n",
       "      <td>-0.083496</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>-0.016968</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.027710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paramaribo</th>\n",
       "      <td>-0.235352</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.154297</td>\n",
       "      <td>0.081055</td>\n",
       "      <td>-0.002716</td>\n",
       "      <td>-0.126953</td>\n",
       "      <td>-0.443359</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>0.038574</td>\n",
       "      <td>-0.063965</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.318359</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.304688</td>\n",
       "      <td>-0.192383</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>-0.341797</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.128906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nuuk</th>\n",
       "      <td>-0.318359</td>\n",
       "      <td>-0.546875</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>-0.167969</td>\n",
       "      <td>-0.376953</td>\n",
       "      <td>-0.453125</td>\n",
       "      <td>-0.332031</td>\n",
       "      <td>-0.447266</td>\n",
       "      <td>-0.105469</td>\n",
       "      <td>-0.024780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>-0.021484</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>-0.294922</td>\n",
       "      <td>-0.226562</td>\n",
       "      <td>0.084473</td>\n",
       "      <td>-0.104980</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>-0.225586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Funafuti</th>\n",
       "      <td>0.087402</td>\n",
       "      <td>0.158203</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>-0.183594</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>-0.277344</td>\n",
       "      <td>-0.285156</td>\n",
       "      <td>-0.160156</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.229492</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223633</td>\n",
       "      <td>0.061279</td>\n",
       "      <td>0.098145</td>\n",
       "      <td>-0.158203</td>\n",
       "      <td>-0.056885</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>-0.337891</td>\n",
       "      <td>0.057861</td>\n",
       "      <td>-0.030029</td>\n",
       "      <td>0.241211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5    \\\n",
       "country    -0.080078  0.133789  0.143555  0.094727 -0.047363 -0.023560   \n",
       "city       -0.010071  0.057373  0.183594 -0.040039 -0.029785 -0.079102   \n",
       "China      -0.073242  0.135742  0.108887  0.083008 -0.127930 -0.227539   \n",
       "Iraq        0.191406  0.125000 -0.065430  0.060059 -0.285156 -0.102539   \n",
       "oil        -0.139648  0.062256 -0.279297  0.063965  0.044434 -0.154297   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "Belmopan   -0.265625 -0.380859 -0.049072  0.155273 -0.044922 -0.248047   \n",
       "Vaduz       0.324219 -0.056885  0.031494 -0.045898  0.042969 -0.062256   \n",
       "Paramaribo -0.235352 -0.063477  0.154297  0.081055 -0.002716 -0.126953   \n",
       "Nuuk       -0.318359 -0.546875  0.085449 -0.167969 -0.376953 -0.453125   \n",
       "Funafuti    0.087402  0.158203  0.028320 -0.183594 -0.046631 -0.277344   \n",
       "\n",
       "                 6         7         8         9    ...       290       291  \\\n",
       "country    -0.008545 -0.186523  0.045898 -0.081543  ... -0.145508  0.067383   \n",
       "city        0.071777  0.013306 -0.143555  0.011292  ...  0.024292 -0.168945   \n",
       "China       0.151367 -0.045654 -0.065430  0.034424  ...  0.140625  0.087402   \n",
       "Iraq        0.117188 -0.351562 -0.095215  0.200195  ... -0.100586 -0.077148   \n",
       "oil        -0.184570 -0.498047  0.047363  0.110840  ... -0.195312 -0.345703   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "Belmopan   -0.010376 -0.105957 -0.328125  0.119629  ...  0.047119 -0.034424   \n",
       "Vaduz       0.004089 -0.328125 -0.151367  0.242188  ... -0.134766 -0.226562   \n",
       "Paramaribo -0.443359 -0.218750  0.038574 -0.063965  ... -0.318359 -0.187500   \n",
       "Nuuk       -0.332031 -0.447266 -0.105469 -0.024780  ...  0.094727 -0.021484   \n",
       "Funafuti   -0.285156 -0.160156  0.064453  0.229492  ... -0.223633  0.061279   \n",
       "\n",
       "                 292       293       294       295       296       297  \\\n",
       "country    -0.244141 -0.077148  0.047607 -0.075195 -0.149414 -0.044189   \n",
       "city       -0.062988  0.117188 -0.020508  0.030273 -0.247070 -0.122559   \n",
       "China       0.152344  0.079590  0.006348 -0.037842 -0.183594  0.137695   \n",
       "Iraq       -0.123047  0.193359 -0.153320  0.089355 -0.173828 -0.054688   \n",
       "oil         0.217773 -0.091797  0.051025  0.061279  0.194336  0.204102   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "Belmopan   -0.005219 -0.265625  0.094727  0.170898 -0.353516  0.072754   \n",
       "Vaduz      -0.083496 -0.152344 -0.179688  0.205078 -0.016968  0.156250   \n",
       "Paramaribo  0.304688 -0.192383  0.050781  0.234375 -0.341797 -0.100098   \n",
       "Nuuk        0.009766 -0.294922 -0.226562  0.084473 -0.104980  0.114746   \n",
       "Funafuti    0.098145 -0.158203 -0.056885  0.235352 -0.337891  0.057861   \n",
       "\n",
       "                 298       299  \n",
       "country     0.097168  0.067383  \n",
       "city        0.076172 -0.234375  \n",
       "China       0.093750 -0.079590  \n",
       "Iraq        0.302734  0.105957  \n",
       "oil         0.235352 -0.051025  \n",
       "...              ...       ...  \n",
       "Belmopan   -0.042969  0.229492  \n",
       "Vaduz       0.152344  0.027710  \n",
       "Paramaribo  0.183594 -0.128906  \n",
       "Nuuk        0.163086 -0.225586  \n",
       "Funafuti   -0.030029  0.241211  \n",
       "\n",
       "[243 rows x 300 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dc31da7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:33:01.830925Z",
     "start_time": "2024-02-04T05:33:01.821743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_word(vec('Italy') + vec('Rome') - vec('Rome'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "### predicting other countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dda5629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:34:18.562945Z",
     "start_time": "2024-02-04T05:34:18.557610Z"
    }
   },
   "outputs": [],
   "source": [
    "capital = vec('France') - vec('Paris')\n",
    "country = vec('Madrid') + capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00922980",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:34:19.109172Z",
     "start_time": "2024-02-04T05:34:19.100412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany\n",
      "China\n"
     ]
    }
   ],
   "source": [
    "print(find_closest_word(vec('Berlin') + capital))\n",
    "print(find_closest_word(vec('Beijing') + capital))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f11afc",
   "metadata": {},
   "source": [
    "### Represent sentence as a vector\n",
    "\n",
    "A whole sentence can be represented as a vector by summing all the word vectors that conform to the sentence. Let us see. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "16318bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T07:17:59.311753Z",
     "start_time": "2024-02-04T07:17:59.298763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02874756,  0.10375977,  0.1326294 ,  0.3330078 , -0.02612305,\n",
       "       -0.5957031 , -0.12597656, -1.0130615 , -0.218544  ,  0.66070557],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"Spain petroleum city king\"\n",
    "vdoc = [vec(x) for x in doc.split(\" \")]\n",
    "doc2vec = np.sum(vdoc, axis = 0)\n",
    "doc2vec[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407250c",
   "metadata": {},
   "source": [
    "### Using gensim library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca70e2f",
   "metadata": {},
   "source": [
    "### Predict the countries from Capitals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c01f3",
   "metadata": {},
   "source": [
    "If you want to download the full dataset on your own and choose your own set of word embeddings,\n",
    "please see the instructions and some helper code.\n",
    "\n",
    "- Download the dataset from this [page](https://code.google.com/archive/p/word2vec/).\n",
    "- Search in the page for 'GoogleNews-vectors-negative300.bin.gz' and click the link to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f427239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T05:39:31.326103Z",
     "start_time": "2024-02-04T05:39:22.426012Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d76c78a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T06:45:42.874071Z",
     "start_time": "2024-02-04T06:45:42.865715Z"
    }
   },
   "outputs": [],
   "source": [
    "# TO download pretrained embeddings trained on google news\n",
    "embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
    "f = open('capitals.txt', 'r').read()\n",
    "set_words = set(nltk.word_tokenize(f))\n",
    "select_words = words = ['king', 'queen', 'oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "for w in select_words:\n",
    "    set_words.add(w)\n",
    "\n",
    "def get_word_embeddings(embeddings):\n",
    "\n",
    "    word_embeddings = {}\n",
    "    for word in embeddings.vocab:\n",
    "        if word in set_words:\n",
    "            word_embeddings[word] = embeddings[word]\n",
    "    return word_embeddings\n",
    "\n",
    "\n",
    "# Testing your function\n",
    "word_embeddings = get_word_embeddings(embeddings)\n",
    "print(len(word_embeddings))\n",
    "pickle.dump( word_embeddings, open( \"word_embeddings_subset.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "912c9037",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T06:46:53.860401Z",
     "start_time": "2024-02-04T06:46:53.812777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city1</th>\n",
       "      <th>country1</th>\n",
       "      <th>city2</th>\n",
       "      <th>country2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bern</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    city1 country1    city2     country2\n",
       "0  Athens   Greece  Bangkok     Thailand\n",
       "1  Athens   Greece  Beijing        China\n",
       "2  Athens   Greece   Berlin      Germany\n",
       "3  Athens   Greece     Bern  Switzerland\n",
       "4  Athens   Greece    Cairo        Egypt"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/sanket.p/python_workbooks/sanket_workbooks/Coursera Courses/Natural-Language-Processing-Specialization-master/Natural Language Processing with Classification and Vector Spaces/Week 3/capitals.txt', delimiter=' ')\n",
    "data.columns = ['city1', 'country1', 'city2', 'country2']\n",
    "\n",
    "# print first five elements in the DataFrame\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e488324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T06:47:01.798484Z",
     "start_time": "2024-02-04T06:47:01.791162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4951, 4)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04410e",
   "metadata": {},
   "source": [
    "### 1.2 Cosine Similarity\n",
    "\n",
    "The cosine similarity function is:\n",
    "\n",
    "$$\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\\tag{1}$$\n",
    "\n",
    "$A$ and $B$ represent the word vectors and $A_i$ or $B_i$ represent index i of that vector.\n",
    "& Note that if A and B are identical, you will get $cos(\\theta) = 1$.\n",
    "* Otherwise, if they are the total opposite, meaning, $A= -B$, then you would get $cos(\\theta) = -1$.\n",
    "* If you get $cos(\\theta) =0$, that means that they are orthogonal (or perpendicular).\n",
    "* Numbers between 0 and 1 indicate a similarity score.\n",
    "* Numbers between -1-0 indicate a dissimilarity score.\n",
    "\n",
    "**Instructions**: Implement a function that takes in two word vectors and computes the cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08ce7a59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T06:52:46.210370Z",
     "start_time": "2024-02-04T06:52:46.198419Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    '''\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: a numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        cos: num representing cosine similarity between A and B\n",
    "    '''\n",
    "    \n",
    "    dot = np.dot(A,B)\n",
    "    normA = np.sqrt(np.dot(A,A))\n",
    "    normB = np.sqrt(np.dot(B,B))\n",
    "    cos = dot / (normA * normB)\n",
    "    \n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8beec79c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T06:52:47.338933Z",
     "start_time": "2024-02-04T06:52:47.327541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6510957"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king = word_embeddings['king']\n",
    "queen = word_embeddings['queen']\n",
    "\n",
    "cosine_similarity(king, queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5056de",
   "metadata": {},
   "source": [
    "### 1.3 Euclidean distance\n",
    "\n",
    "You will now implement a function that computes the similarity between two vectors using the Euclidean distance.\n",
    "Euclidean distance is defined as:\n",
    "\n",
    "$$ \\begin{aligned} d(\\mathbf{A}, \\mathbf{B})=d(\\mathbf{B}, \\mathbf{A}) &=\\sqrt{\\left(A_{1}-B_{1}\\right)^{2}+\\left(A_{2}-B_{2}\\right)^{2}+\\cdots+\\left(A_{n}-B_{n}\\right)^{2}} \\\\ &=\\sqrt{\\sum_{i=1}^{n}\\left(A_{i}-B_{i}\\right)^{2}} \\end{aligned}$$\n",
    "\n",
    "* $n$ is the number of elements in the vector\n",
    "* $A$ and $B$ are the corresponding word vectors. \n",
    "* The more similar the words, the more likely the Euclidean distance will be close to 0. \n",
    "\n",
    "**Instructions**: Write a function that computes the Euclidean distance between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44a565b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T06:53:49.208827Z",
     "start_time": "2024-02-04T06:53:49.203436Z"
    }
   },
   "outputs": [],
   "source": [
    "def euclidean(A, B):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: A numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        d: numerical number representing the Euclidean distance between A and B.\n",
    "    \"\"\"\n",
    "    \n",
    "    d = np.linalg.norm(A - B)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33d01cb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T06:53:53.743229Z",
     "start_time": "2024-02-04T06:53:53.736953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4796925"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(king, queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9efc2f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T07:14:44.612376Z",
     "start_time": "2024-02-04T07:14:44.602982Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_country(city1, country1, city2, embeddings):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        city1: a string (the capital city of country1)\n",
    "        country1: a string (the country of capital1)\n",
    "        city2: a string (the capital city of country2)\n",
    "        embeddings: a dictionary where the keys are words and values are their embeddings\n",
    "    Output:\n",
    "        countries: a dictionary with the most likely country and its similarity score\n",
    "    \"\"\"\n",
    "    \n",
    "    # storing city1, country1, and city2 in set \n",
    "    group = set((city1, country1, city2))\n",
    "    \n",
    "    # get embeddings of city 1, country1, city2\n",
    "    city1_emb = word_embeddings[city1]\n",
    "    country1_emb = word_embeddings[country1]\n",
    "    city2_emb = word_embeddings[city2]\n",
    "    \n",
    "    # Equation to get embedding for country2\n",
    "    vec = country1_emb - city1_emb + city2_emb\n",
    "    \n",
    "    # Initialize the similarity to -1 \n",
    "    similarity = -1\n",
    "    \n",
    "    # initialize country to an empty string\n",
    "    country = ''\n",
    "    \n",
    "    for word in word_embeddings.keys():\n",
    "    \n",
    "        # TO check if word is not already not in 'group'\n",
    "        if word not in group:\n",
    "            \n",
    "            word_emb = word_embeddings[word]\n",
    "            \n",
    "            cos_similarity = cosine_similarity(vec, word_emb)\n",
    "            \n",
    "            if cos_similarity > similarity:\n",
    "                \n",
    "                similarity = cos_similarity\n",
    "                \n",
    "                country = (word, similarity)\n",
    "                \n",
    "    return country\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee32578f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T07:14:45.892597Z",
     "start_time": "2024-02-04T07:14:45.872147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Egypt', 0.7626821)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_country('Athens', 'Greece', 'Cairo', word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1485d566",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T07:15:44.848438Z",
     "start_time": "2024-02-04T07:15:44.845557Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(word_embeddings, data):\n",
    "    '''\n",
    "    Input:\n",
    "        word_embeddings: a dictionary where the key is a word and the value is its embedding\n",
    "        data: a pandas dataframe containing all the country and capital city pairs\n",
    "    \n",
    "    Output:\n",
    "        accuracy: the accuracy of the model\n",
    "    '''\n",
    "    \n",
    "    num_correct = 0\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        \n",
    "        city1, country1, city2, country2 = row['city1'], row['country1'], row['city2'], row['country2']\n",
    "#         print(city1, country1, city2, country2)\n",
    "        predicted_country2, _ = get_country(city1, country1, city2, word_embeddings)\n",
    "        \n",
    "        if predicted_country2 == country2:\n",
    "            num_correct +=1\n",
    "            \n",
    "    m = len(data)\n",
    "\n",
    "    accuracy = num_correct/m\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de4ecc8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T07:15:52.282965Z",
     "start_time": "2024-02-04T07:15:46.209300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.92\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(word_embeddings, data)\n",
    "print(f\"Accuracy is {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f80545cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T07:14:26.710643Z",
     "start_time": "2024-02-04T07:14:26.686449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city1</th>\n",
       "      <th>country1</th>\n",
       "      <th>city2</th>\n",
       "      <th>country2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bern</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Cairo</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4946</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>Dakar</td>\n",
       "      <td>Senegal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>Damascus</td>\n",
       "      <td>Syria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>Dhaka</td>\n",
       "      <td>Bangladesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>Doha</td>\n",
       "      <td>Qatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>Ireland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4951 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       city1 country1     city2     country2\n",
       "0     Athens   Greece   Bangkok     Thailand\n",
       "1     Athens   Greece   Beijing        China\n",
       "2     Athens   Greece    Berlin      Germany\n",
       "3     Athens   Greece      Bern  Switzerland\n",
       "4     Athens   Greece     Cairo        Egypt\n",
       "...      ...      ...       ...          ...\n",
       "4946  Zagreb  Croatia     Dakar      Senegal\n",
       "4947  Zagreb  Croatia  Damascus        Syria\n",
       "4948  Zagreb  Croatia     Dhaka   Bangladesh\n",
       "4949  Zagreb  Croatia      Doha        Qatar\n",
       "4950  Zagreb  Croatia    Dublin      Ireland\n",
       "\n",
       "[4951 rows x 4 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503d1192",
   "metadata": {},
   "source": [
    "# Plotting vectors using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5eb7f",
   "metadata": {},
   "source": [
    "Now you will explore the distance between word vectors after\n",
    "reducing their dimension.\n",
    "The technique we will employ is known as\n",
    "[*principal component analysis* (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "As we saw, we are working in a 300-dimensional space in this case.\n",
    "Although from a computational perspective we were able to perform a good job,\n",
    "it is impossible to visualize results in such high dimensional spaces.\n",
    "\n",
    "You can think of PCA as a method that projects our vectors in a space of reduced\n",
    "dimension, while keeping the maximum information about the original vectors in\n",
    "their reduced counterparts. In this case, by *maximum infomation* we mean that the\n",
    "Euclidean distance between the original vectors and their projected siblings is\n",
    "minimal. Hence vectors that were originally close in the embeddings dictionary,\n",
    "will produce lower dimensional vectors that are still close to each other.\n",
    "\n",
    "You will see that when you map out the words, similar words will be clustered\n",
    "next to each other. For example, the words 'sad', 'happy', 'joyful' all describe\n",
    "emotion and are supposed to be near each other when plotted.\n",
    "The words: 'oil', 'gas', and 'petroleum' all describe natural resources.\n",
    "Words like 'city', 'village', 'town' could be seen as synonyms and describe a\n",
    "similar thing.\n",
    "\n",
    "Before plotting the words, you need to first be able to reduce each word vector\n",
    "with PCA into 2 dimensions and then plot it. The steps to compute PCA are as follows:\n",
    "\n",
    "1. Mean normalize the data\n",
    "2. Compute the covariance matrix of your data ($\\Sigma$). \n",
    "3. Compute the eigenvectors and the eigenvalues of your covariance matrix\n",
    "4. Multiply the first K eigenvectors by your normalized data. The transformation should look something as follows:\n",
    "\n",
    "<!-- <img src = 'word_embf.jpg' width=\"width\" height=\"height\" style=\"width:800px;height:200px;\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ae0ac8",
   "metadata": {},
   "source": [
    "**Steps to write PCA function**: \n",
    "\n",
    "You will write a program that takes in a data set where each row corresponds to a word vector. \n",
    "* The word vectors are of dimension 300. \n",
    "* Use PCA to change the 300 dimensions to `n_components` dimensions. \n",
    "* The new matrix should be of dimension `m, n_componentns`. \n",
    "\n",
    "* First de-mean the data\n",
    "* Get the eigenvalues using `linalg.eigh`.  Use `eigh` rather than `eig` since R is symmetric.  The performance gain when using `eigh` instead of `eig` is substantial.\n",
    "* Sort the eigenvectors and eigenvalues by decreasing order of the eigenvalues.\n",
    "* Get a subset of the eigenvectors (choose how many principle components you want to use using `n_components`).\n",
    "* Return the new transformation of the data by multiplying the eigenvectors with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "63ee2881",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T07:35:13.499637Z",
     "start_time": "2024-02-04T07:35:13.486124Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_pca(X, n_components = 2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X: of dimension (m, n) where each row corresponds to word vector\n",
    "        n_components: Number of compoments you want to keep (reduced dimension)\n",
    "    Output:\n",
    "        X_reduced: data transformed \n",
    "    \"\"\"\n",
    "    \n",
    "    # mean center the data\n",
    "    X_demeaned = X - np.mean(X, axis=0)\n",
    "    \n",
    "    # Calculate covariance matrix\n",
    "    covariance_matrix = np.cov(X_demeaned, rowvar=False)\n",
    "    \n",
    "    # calculate eigenvectors & eigen values of covariance matrix\n",
    "    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix, UPLO='L')\n",
    "    \n",
    "    # sort eigenvalue in increasing order (get the indecess from sort)\n",
    "    idx_sorted = np.argsort(eigen_vals)\n",
    "    \n",
    "    # reverse the order so that its from highest to lowest\n",
    "    idx_sorted_decreasing = idx_sorted[::-1]\n",
    "    \n",
    "    # sort the eigen values by idx_sorted_decreasing\n",
    "    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n",
    "    \n",
    "    # sort eigenvectors using the idx_sorted_decreasing indices\n",
    "    eigen_vecs_sorted = eigen_vecs[:,idx_sorted_decreasing]\n",
    "    \n",
    "    # select the first n eigenvectors (n is desired dimension\n",
    "    # of rescaled data array, or dims_rescaled_data)\n",
    "    eigen_vecs_subset = eigen_vecs_sorted[:,0:n_components]\n",
    "    \n",
    "    # transform the data by multiplying the transpose of the eigenvectors \n",
    "    # with the transpose of the de-meaned data\n",
    "    # Then take the transpose of that product.\n",
    "    X_reduced = np.dot(eigen_vecs_subset.transpose(),X_demeaned.transpose()).transpose()\n",
    "\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8c02ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T07:35:14.124183Z",
     "start_time": "2024-02-04T07:35:14.111526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your original matrix was (3, 10) and it became:\n",
      "[[ 0.43437323  0.49820384]\n",
      " [ 0.42077249 -0.50351448]\n",
      " [-0.85514571  0.00531064]]\n"
     ]
    }
   ],
   "source": [
    "# Testing  function\n",
    "np.random.seed(1)\n",
    "X = np.random.rand(3, 10)\n",
    "X_reduced = compute_pca(X, n_components=2)\n",
    "print(\"Your original matrix was \" + str(X.shape) + \" and it became:\")\n",
    "print(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d9513cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T07:37:00.546963Z",
     "start_time": "2024-02-04T07:37:00.536727Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vectors(embeddings, words):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        embeddings: a word \n",
    "        fr_embeddings:\n",
    "        words: a list of words\n",
    "    Output: \n",
    "        X: a matrix where the rows are the embeddings corresponding to the rows on the list\n",
    "        \n",
    "    \"\"\"\n",
    "    m = len(words)\n",
    "    X = np.zeros((1, 300))\n",
    "    for word in words:\n",
    "        english = word\n",
    "        eng_emb = embeddings[english]\n",
    "        X = np.row_stack((X, eng_emb))\n",
    "    X = X[1:,:]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9f7fd906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T07:37:28.826080Z",
     "start_time": "2024-02-04T07:37:28.659363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using PCA to plot few words\n",
    "\n",
    "words = ['oil', 'gas', 'happy', 'sad', 'city', 'town',\n",
    "         'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "\n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "X = get_vectors(word_embeddings, words)\n",
    "\n",
    "result = compute_pca(X, 2)\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0] - 0.05, result[i, 1] + 0.1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print('You have 11 words each of 300 dimensions thus X.shape is:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26926406",
   "metadata": {},
   "source": [
    "The word vectors for 'gas', 'oil' and 'petroleum' appear related to each other,\n",
    "because their vectors are close to each other.  Similarly, 'sad', 'joyful'\n",
    "and 'happy' all express emotions, and are also near each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08900fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
